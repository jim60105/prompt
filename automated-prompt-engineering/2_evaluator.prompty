---
name: Automated Prompt Engineering
description: ğŸ‘‹ I'm here to help you create effective prompts! ğŸ¯ 
model:
  parameters:
    response_format:
      type: json_object
outputs:
  explanation:
    type: string
    description: Explanation of how you will rate the response.
  score:
    type: float
    description: Rate the above response on a scale from 0 to 100, where 0 is poor and 100 is excellent
sample:
  scoring_criteria: |
    0-20: 
    Criteria: Response is incomplete or irrelevant to the goal. 
    Indicators: Lacks essential components, off-topic, poor language use. 
    Pitfalls: Ignoring user instructions, minimal effort. 
    
    21-40: 
    Criteria: Response is partially complete but lacks depth and clarity. 
    Indicators: Some relevant content, basic structure, minor language issues. 
    Pitfalls: Superficial treatment of topics, vague suggestions. 
    
    41-60: 
    Criteria: Response meets basic requirements but needs improvement in detail and coherence. 
    Indicators: Covers main points, acceptable language, some structure. 
    Pitfalls: Missing key details, occasional unclear sections. 
    
    61-80: 
    Criteria: Response is well-structured and mostly complete, with good detail. 
    Indicators: Clear and relevant content, good language use, logical flow. 
    Pitfalls: Minor omissions, occasional minor errors. 
    
    81-100: 
    Criteria: Response is comprehensive, highly relevant, and well-articulated. 
    Indicators: Detailed and thorough, excellent language use, coherent and logical. 
    Pitfalls: Overlooking minor aspects, rare minor errors.
    
    Adjusting Criteria: 
    - For complex goals, increase weight on detail and coherence. 
    - For simpler goals, focus on relevance and clarity. 
    
    Weighing Aspects: 
    - Content relevance: 40% 
    - Detail and depth: 30% 
    - Language and clarity: 20% 
    - Structure and coherence: 10% 
    
    Score Boundaries: 
    - 0-20: Incomplete/irrelevant 
    - 21-40: Partial/unclear 
    - 41-60: Basic/needs improvement 
    - 61-80: Good/mostly complete 
    - 81-100: Excellent/comprehensive 
    
    Checklist: 
    - Objective: Relevance, completeness, accuracy 
    - Subjective: Clarity, coherence, language quality 
    
    Quick-Reference Guide: 
    - 0-20: Poor 
    - 21-40: Fair 
    - 41-60: Satisfactory 
    - 61-80: Good 
    - 81-100: Excellent 
    
    Calibration: 
    - Sample Responses: Provide examples for each score range 
    - Tips: Regularly review criteria, use peer reviews for consistency.
  user_input: |
    å¯«ä¸€ç¯‡éƒ¨è½æ ¼ç°¡ä»‹ä»‹ç´¹ åœ¨ RHEL 8.8 é›¢ç·šå®‰è£ Dockerï¼Œé€™ç¯‡æ–‡ç« æœƒåˆ†äº«æ•´å€‹å®‰è£æµç¨‹ï¼Œæˆ‘éœ€è¦ä½ å¹«æˆ‘æ’°å¯«å‰è¨€
  answer: |
    å¥½çš„ï¼Œæˆ‘å¾ˆæ¨‚æ„å”åŠ©æ‚¨æ’°å¯«é€™ç¯‡é—œæ–¼åœ¨ RHEL 8.8 é›¢ç·šå®‰è£ Docker çš„éƒ¨è½æ ¼æ–‡ç« ã€‚é¦–å…ˆï¼Œè®“æˆ‘ç¢ºèªä¸€ä¸‹æ‚¨æä¾›çš„è³‡è¨Šï¼š
    
    <blog_topic>åœ¨ RHEL 8.8 é›¢ç·šå®‰è£ Docker</blog_topic>
    <specific_points>é€™ç¯‡æ–‡ç« æœƒåˆ†äº«æ•´å€‹å®‰è£æµç¨‹</specific_points>
    
    åŸºæ–¼é€™äº›è³‡è¨Šï¼Œæˆ‘æœƒç‚ºæ‚¨æ’°å¯«ä¸€å€‹å‰è¨€ã€‚åœ¨é–‹å§‹ä¹‹å‰ï¼Œè®“æˆ‘å…ˆæä¾›ä¸€å€‹ç°¡å–®çš„å¤§ç¶±å»ºè­°ï¼Œä»¥ç¢ºä¿æˆ‘å€‘çš„æ–‡ç« çµæ§‹æ¸…æ™°ï¼š
    
    <outline>
    # åœ¨ RHEL 8.8 é›¢ç·šç’°å¢ƒä¸­å®‰è£ Dockerï¼šå®Œæ•´æŒ‡å—
    
    ## å‰è¨€
    ## Docker ç°¡ä»‹åŠå…¶åœ¨é›¢ç·šç’°å¢ƒä¸­çš„é‡è¦æ€§
    ## æº–å‚™å·¥ä½œ
    ### ç³»çµ±éœ€æ±‚
    ### å¿…è¦çš„å®‰è£åŒ…
    ## é›¢ç·šå®‰è£æ­¥é©Ÿ
    ### ä¸‹è¼‰æ‰€éœ€çš„ RPM åŒ…
    ### å‚³è¼¸æ–‡ä»¶åˆ°ç›®æ¨™ç³»çµ±
    ### å®‰è£ Docker
    ## é©—è­‰å®‰è£
    ## å¸¸è¦‹å•é¡ŒåŠè§£æ±ºæ–¹æ¡ˆ
    ## ç¸½çµ
    </outline>
    
    ç¾åœ¨ï¼Œè®“æˆ‘ç‚ºæ‚¨æ’°å¯«å‰è¨€éƒ¨åˆ†ï¼š
    
    åœ¨ç•¶ä»Šå¿«é€Ÿç™¼å±•çš„ IT ç’°å¢ƒä¸­ï¼Œå®¹å™¨æŠ€è¡“å·²æˆç‚ºæ‡‰ç”¨ç¨‹å¼é–‹ç™¼å’Œéƒ¨ç½²çš„é‡è¦å·¥å…·ã€‚è€Œ Dockerï¼Œä½œç‚ºå®¹å™¨åŒ–æŠ€è¡“çš„å…ˆé©…ï¼Œæ›´æ˜¯ä¸å¯æˆ–ç¼ºçš„å¹³å°ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘å¯èƒ½éœ€è¦åœ¨é›¢ç·šç’°å¢ƒä¸­é€²è¡Œå®‰è£ï¼Œé€™å°æ–¼å®‰å…¨æ€§è¦æ±‚é«˜æˆ–ç¶²çµ¡å—é™çš„ç’°å¢ƒä¾†èªªå°¤ç‚ºé‡è¦ã€‚
    
    æœ¬æ–‡å°‡ç‚ºä½ è©³ç´°ä»‹ç´¹å¦‚ä½•åœ¨ Red Hat Enterprise Linuxï¼ˆRHELï¼‰8.8 çš„é›¢ç·šç’°å¢ƒä¸­å®‰è£ Dockerã€‚ç„¡è«–ä½ æ˜¯å› ç‚ºå®‰å…¨è€ƒæ…®ã€ç¶²çµ¡é™åˆ¶ï¼Œé‚„æ˜¯simplyæƒ³è¦æ›´å¥½åœ°æ§åˆ¶å®‰è£éç¨‹ï¼Œé€™ç¯‡æŒ‡å—éƒ½èƒ½å¹«åŠ©ä½ é †åˆ©å®Œæˆ Docker çš„å®‰è£ã€‚
    
    åœ¨æ¥ä¸‹ä¾†çš„å…§å®¹ä¸­ï¼Œæˆ‘æœƒå¸¶é ˜å¤§å®¶ä¸€æ­¥æ­¥äº†è§£æ•´å€‹å®‰è£æµç¨‹ï¼Œå¾æº–å‚™å·¥ä½œåˆ°æœ€çµ‚çš„é©—è­‰æ­¥é©Ÿï¼Œæ¯å€‹ç’°ç¯€éƒ½æœƒè©³ç´°èªªæ˜ã€‚æˆ‘å€‘é‚„æœƒè¨è«–ä¸€äº›å¸¸è¦‹çš„å•é¡ŒåŠå…¶è§£æ±ºæ–¹æ¡ˆï¼Œç¢ºä¿ä½ èƒ½å¤ é †åˆ©åœ°åœ¨ RHEL 8.8 ç³»çµ±ä¸Šé‹è¡Œ Dockerã€‚
    
    ç„¡è«–ä½ æ˜¯ç³»çµ±ç®¡ç†å“¡ã€é–‹ç™¼è€…ï¼Œé‚„æ˜¯å° Docker æŠ€è¡“æ„Ÿèˆˆè¶£çš„ IT æ„›å¥½è€…ï¼Œé€™ç¯‡æ–‡ç« éƒ½èƒ½ç‚ºä½ æä¾›å¯¶è²´çš„åƒè€ƒã€‚è®“æˆ‘å€‘é–‹å§‹é€™æ®µé›¢ç·šå®‰è£ Docker çš„ç²¾å½©æ—…ç¨‹å§ï¼
    
    é€™å€‹å‰è¨€å¦‚ä½•ï¼Ÿå®ƒæ¦‚è¿°äº†æ–‡ç« çš„ä¸»é¡Œã€ç›®çš„å’Œé‡è¦æ€§ï¼ŒåŒæ™‚ä¹Ÿçµ¦è®€è€…ä¸€å€‹å°æ•´ç¯‡æ–‡ç« å…§å®¹çš„é æœŸã€‚å¦‚æœä½ æƒ³è¦é€²è¡Œä»»ä½•ä¿®æ”¹æˆ–å¼·èª¿ç‰¹å®šæ–¹é¢ï¼Œè«‹å‘Šè¨´æˆ‘ï¼Œæˆ‘æœƒå¾ˆæ¨‚æ„é€²è¡Œèª¿æ•´ã€‚
---

system:
- Carefully consider the user's question to ensure your answer is logical and makes sense.
- Make sure your explanation is concise and easy to understand, not verbose.
- Strictly return the answer in json format.
- Strictly Ensure that the following answer is in a valid JSON format.
- The output should be formatted as a JSON instance that conforms to the JSON schema below and do not add comments.

Here is the output schema:
'''
{
	"explanation": string //Explanation of how you will rate the response.
	"score": float //Rate the above response on a scale from 0 to 100, where 0 is poor and 100 is excellent
}
'''
<EXPLANATION>
You are a review model tasked with evaluating the correctness of a response to a problem. 

{{scoring_criteria}}
</EXPLANATION>

<QUESTION>
{{user_input}}
</QUESTION>

<TASK>
Please rate the response provided by user on a scale from 0 to 100, where 0 is poor and 100 is excellent. Provide an explanation for your rating. Tend to be stricter when rating.
</TASK>

user:
<RESPONSES_TO_BE_RATED>
{{answer}}
</RESPONSES_TO_BE_RATED>
